# API CONFIGURATION
# URL where the API can be accessed by the browser
# This setting allows the frontend to connect to the API at runtime (no rebuild needed!)
#
# IMPORTANT: Do NOT include /api at the end - it will be added automatically!
#
# Common scenarios:
# - Docker on localhost: http://localhost:8502 (recommended - uses built-in proxy)
# - Docker on LAN/remote server: https://notebook.example.com or http://your-server-ip:8502
# - Behind reverse proxy with custom domain: https://your-domain.com
# - Behind reverse proxy with subdomain: https://api.your-domain.com
#
# Examples for reverse proxy users:
# - API_URL=https://notebook.example.com  (frontend will call https://notebook.example.com/api/*)
# - API_URL=https://api.example.com       (frontend will call https://api.example.com/api/*)
#
# Note: If not set, the system will auto-detect based on the incoming request.
# Leave this unset for most deployments, especially when users connect from another machine.
# Only set this if you need to override the auto-detection (e.g., reverse proxy scenarios).
# For remote users, set this to the exact URL your browser uses (https://notebook.example.com or http://server-ip:8502).
# API_URL=https://your-domain-or-ip

# INTERNAL API URL (Server-Side)
# URL where Next.js server-side should proxy API requests (via rewrites)
# This is DIFFERENT from API_URL which is used by the browser client
#
# INTERNAL_API_URL is used by Next.js rewrites to forward /api/* requests to the FastAPI backend
# API_URL is used by the browser to know where to make API calls
#
# Default: http://localhost:5055 (single-container deployment - both services on same host)
# Override for multi-container: INTERNAL_API_URL=http://api-service:5055
#
# Common scenarios:
# - Single container (default): Don't set - defaults to http://localhost:5055
# - Multi-container Docker Compose: INTERNAL_API_URL=http://api:5055 (use service name)
# - Kubernetes/advanced networking: INTERNAL_API_URL=http://api-service.namespace.svc.cluster.local:5055
#
# Why two variables?
# - API_URL: External/public URL that browsers use (can be https://your-domain.com)
# - INTERNAL_API_URL: Internal container networking URL (usually http://localhost:5055 or service name)
#
# INTERNAL_API_URL=http://localhost:5055

# API CLIENT TIMEOUT (in seconds)
# Controls how long the frontend/Streamlit UI waits for API responses
# Increase this if you're using slow AI providers or hardware (Ollama on CPU, remote LM Studio, etc.)
# Default: 300 seconds (5 minutes) - sufficient for most transformation/insight operations
#
# Common scenarios:
# - Fast cloud APIs (OpenAI, Anthropic): 300 seconds is more than enough
# - Local Ollama on GPU: 300 seconds should work fine
# - Local Ollama on CPU: Consider 600 seconds (10 minutes) or more
# - Remote LM Studio over slow network: Consider 900 seconds (15 minutes)
# - Very large documents: May need 900+ seconds
#
# API_CLIENT_TIMEOUT=300

# ESPERANTO LLM TIMEOUT (in seconds)
# Controls the timeout for AI model API calls at the Esperanto library level
# This is separate from API_CLIENT_TIMEOUT and applies to the actual LLM provider requests
# Only increase this if you're experiencing timeouts during model inference itself
# Default: 60 seconds (built into Esperanto)
#
# Important: This should generally be LOWER than API_CLIENT_TIMEOUT to allow proper error handling
#
# Common scenarios:
# - Fast cloud APIs (OpenAI, Anthropic, Groq): 60 seconds is sufficient
# - Local Ollama with small models: 120-180 seconds may help
# - Local Ollama with large models on CPU: 300+ seconds
# - Remote or self-hosted LLMs: 180-300 seconds depending on hardware
#
# Note: If transformations complete but you see timeout errors, increase API_CLIENT_TIMEOUT first.
# Only increase ESPERANTO_LLM_TIMEOUT if the model itself is timing out during inference.
#
# ESPERANTO_LLM_TIMEOUT=60

# SECURITY
# Set this to protect your Open Notebook instance with a password (for public hosting)
# OPEN_NOTEBOOK_PASSWORD=
# Session secret used for API session middleware (required)
SESSION_SECRET_KEY=change-me-to-a-random-string
# Optional: Override Google OAuth redirect callback (defaults to API_URL or localhost)
# GOOGLE_REDIRECT_URI=https://notebook.example.com/api/oauth2/google/callback

# OPENAI
OPENAI_API_KEY=sk-your-openai-api-key-here

# ANTHROPIC
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# GEMINI
# this is the best model for long context and podcast generation
# GOOGLE_API_KEY=your-google-api-key-here
# GEMINI_API_BASE_URL=  # Optional: Override default endpoint (for Vertex AI, proxies, etc.)

# VERTEXAI
# VERTEX_PROJECT=my-google-cloud-project-name
# GOOGLE_APPLICATION_CREDENTIALS=./google-credentials.json
# VERTEX_LOCATION=us-east5

# MISTRAL
# MISTRAL_API_KEY=your-mistral-api-key-here

# DEEPSEEK
# DEEPSEEK_API_KEY=sk-your-deepseek-api-key-here

# OLLAMA
# OLLAMA_API_BASE="http://localhost:11434"

# OPEN ROUTER
# OPENROUTER_BASE_URL="https://openrouter.ai/api/v1"
# OPENROUTER_API_KEY=sk-your-openrouter-api-key-here

# GROQ
# GROQ_API_KEY=gsk_your-groq-api-key-here

# XAI
# XAI_API_KEY=your-xai-api-key-here

# ELEVENLABS
# Used only by the podcast feature
# ELEVENLABS_API_KEY=your-elevenlabs-api-key-here

# TTS BATCH SIZE
# Controls concurrent TTS requests for podcast generation (default: 5)
# Lower values reduce provider load but increase generation time
# Recommended: OpenAI=5, ElevenLabs=2, Google=4, Custom=1
# TTS_BATCH_SIZE=2

# VOYAGE AI
# VOYAGE_API_KEY=pa-your-voyage-api-key-here

# OPENAI COMPATIBLE ENDPOINTS
# Generic configuration (applies to all modalities: language, embedding, STT, TTS)
# OPENAI_COMPATIBLE_BASE_URL=http://localhost:1234/v1
# OPENAI_COMPATIBLE_API_KEY=your-api-key-here

# Mode-specific configuration (overrides generic if set)
# Use these when you want different endpoints for different capabilities
# OPENAI_COMPATIBLE_BASE_URL_LLM=http://localhost:1234/v1
# OPENAI_COMPATIBLE_API_KEY_LLM=your-llm-api-key-here
# OPENAI_COMPATIBLE_BASE_URL_EMBEDDING=http://localhost:8080/v1
# OPENAI_COMPATIBLE_API_KEY_EMBEDDING=your-embedding-api-key-here
# OPENAI_COMPATIBLE_BASE_URL_STT=http://localhost:9000/v1
# OPENAI_COMPATIBLE_API_KEY_STT=your-stt-api-key-here
# OPENAI_COMPATIBLE_BASE_URL_TTS=http://localhost:9000/v1
# OPENAI_COMPATIBLE_API_KEY_TTS=your-tts-api-key-here

# AZURE OPENAI
# AZURE_OPENAI_API_KEY=your-azure-openai-api-key-here
# AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/
# AZURE_OPENAI_API_VERSION="2024-12-01-preview"
# AZURE_OPENAI_DEPLOYMENT_NAME=your-deployment-name
# Core Azure Configuration
# AZURE_OPENAI_API_KEY=your-azure-openai-api-key-here
# AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/
# AZURE_OPENAI_API_VERSION=2024-12-01-preview

# Multi-deployment configuration
# IMPORTANT: Replace these with your actual deployment names from Azure AI Studio
# AZURE_OPENAI_CHAT_DEPLOYMENT=your-chat-deployment-name
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT=your-embedding-deployment-name

# USE THIS IF YOU WANT TO DEBUG THE APP ON LANGSMITH
# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
# LANGCHAIN_API_KEY=ls-your-langsmith-api-key-here
# LANGCHAIN_PROJECT="Open Notebook"

# CONNECTION DETAILS FOR YOUR SURREAL DB
# New format (preferred) - WebSocket URL
# Local development (all services on same machine): ws://localhost:8000/rpc
# Docker Compose (separate API + SurrealDB services): ws://surrealdb:8000/rpc
SURREAL_URL="ws://localhost:8000/rpc"
SURREAL_USER="root"
SURREAL_PASSWORD="root"
SURREAL_NAMESPACE="open_notebook"
SURREAL_DATABASE="main"

# FIRECRAWL - Get a key at https://firecrawl.dev/
# FIRECRAWL_API_KEY=fc-your-firecrawl-api-key-here

# JINA - Get a key at https://jina.ai/
# JINA_API_KEY=your-jina-api-key-here

# API Configuration
# API_HOST controls which network interface the FastAPI server binds to
# - 127.0.0.1 (default): Only accessible from localhost
# - 0.0.0.0: Accessible from any network interface (required for remote access)
API_HOST=127.0.0.1
API_PORT=5055

# API_URL: External URL that browsers use to access the API
# IMPORTANT: This should match your public IP or domain for remote access
# API_URL=http://localhost:5055
# NEXT_PUBLIC_API_URL=http://localhost:5055